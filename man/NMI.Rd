% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/mutual_information.R
\name{NMI}
\alias{NMI}
\title{Normalized Mutual information estimator}
\usage{
NMI(imA, imB, binsA = 10, binsB = 10)
}
\arguments{
\item{imA, imB}{cimg objects}

\item{binsA, binsB}{numerical values indicatin the number of discrete bins for each image}
}
\value{
the NMI value for imA and imB
}
\description{
Function used to estimate the Normalized Mutual Information
}
\details{
In probability theory and information theory, the mutual information (MI)
 of two random variables is a measure of the mutual dependence between the
 two variables. More specifically, it quantifies the "amount of information"
 (in units such as bits) obtained about one random variable, through the
 other random variable. The concept of mutual information is intricately
 linked to that of entropy of a random variable, a fundamental notion in
 information theory, that defines the "amount of information" held in a
 random variable.
}
\examples{
imA <- ground[[1]]
imB <- ground[[2]]
imC <- ground[[20]]
NMI(imA, imB)
NMI(imA, imC)
entropy <- sapply(ground, NMI, imB = ground[[1]])
plot(entropy)
lines(entropy)
}
